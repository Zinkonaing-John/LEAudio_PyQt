# MCP (Minimum Compelling Product) — plan + Step-1 prototype (PyQt5)

Nice — let’s build a focused, testable MCP you can expand later. I’ll (A) give a compact MCP plan, (B) list milestones, and (C) deliver **Step 1**: a runnable PyQt5 prototype that records audio, shows the recognized text, calls a free public translation API (LibreTranslate) and shows the translation. I’ll also include a Play (TTS) action. I’ll point out stronger offline/production options for later steps.

Quick tool references I used:

* LibreTranslate — free / open-source translation API (can run your own instance or use hosted). ([libretranslate.com][1])
* LibreTranslate portal & rate info (public hosted limits / API key). ([portal.libretranslate.com][2])
* Vosk — offline speech recognition that supports Korean for an offline option. ([Alpha Cephei][3])
* Python `SpeechRecognition` library (Google Web Speech option for quick prototype). ([PyPI][4])
* `gTTS` (Google TTS wrapper) supports `ko` (Korean) and many languages — good for quick prototyping TTS. ([PyPI][5])

---

# A. MCP summary (one-paragraph)

A **Minimum Compelling Product**: a desktop PyQt5 app with a single “Record” button, a language selector, a textbox showing the recognized (Korean) text, automatic call to a free translation API to translate into the chosen target language, and a Play button to speak the translated result. This lets end users try the core flow (speech → text → translate → speech) with minimal infra. (Later you’ll replace public/free services with your own backend API for better quality and scaling.)

---

# B. Core features for the MCP

1. UI: one big **Record** button (start/stop), a **Target language** dropdown, original text viewer, translated text viewer, and **Play** (TTS) button.
2. STT (prototype): use local/quick online recognizer (Google via `SpeechRecognition`) to get Korean text fast. (Later: Vosk/Whisper for offline/accuracy). ([PyPI][4])
3. Translation: call LibreTranslate public endpoint for free translation now (or self-host later). ([libretranslate.com][1])
4. TTS: use `gTTS` to produce playable audio for the target language (quick prototype). ([PyPI][5])
5. Language control: dropdown with language codes (you control source/target; default source = Korean).
6. Basic error handling, busy UI states, and temporary audio files for TTS playback.

---

# C. Milestones (step-by-step)

1. **Step 1 (Now / MCP)** — PyQt5 app: Record → STT → translate via LibreTranslate → show translation → Play TTS. (I’ll provide full code below.)
2. **Step 2** — Swap STT to an offline model (Vosk) or Whisper (better accuracy), and optionally self-host translation (LibreTranslate). ([Alpha Cephei][3])
3. **Step 3** — Replace prototype TTS with higher-quality voices (Coqui / commercial cloud or self-hosted models). ([GitHub][6])
4. **Step 4** — Build backend API: centralize STT/translation/TTS, add auth & rate limits, caching, and logging. (Mobile/web clients call this.)
5. **Step 5** — UX polish: streaming, real-time partial transcripts, language detection, noise robustness, packaging.

---

# D. **Step 1: runnable PyQt5 prototype**

Copy + run. This prototype uses:

* `PyQt5` for UI
* `SpeechRecognition` + default Google web recognizer (quick, no API key for prototype)
* `requests` to call LibreTranslate public endpoint
* `gTTS` + `playsound` to play TTS

**Install dependencies** (recommended virtualenv):

```bash
pip install PyQt5 SpeechRecognition pyaudio requests gTTS playsound==1.3.0
```

**Notes**:

* On some systems `pyaudio` is tricky to install (you may need system packages: `portaudio`/`brew install portaudio` or `apt install portaudio19-dev`), or use prebuilt wheels. If `pyaudio` is difficult, later we can swap to `sounddevice` recording.
* LibreTranslate: public hosted instance works for prototyping but has limits; for heavy use self-host. ([libretranslate.com][1])

---

### The Python code (PyQt5 app)

```python
# save as stt_translate_pyqt.py
import sys
import os
import tempfile
import threading
import requests
from gtts import gTTS
from playsound import playsound

from PyQt5.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout,
    QPushButton, QTextEdit, QLabel, QComboBox
)
from PyQt5.QtCore import Qt, pyqtSignal, QObject

import speech_recognition as sr

# -----------------
# Simple signals helper
# -----------------
class Signals(QObject):
    transcription_ready = pyqtSignal(str)
    translation_ready = pyqtSignal(str)
    set_status = pyqtSignal(str)

signals = Signals()

# -----------------
# Config / language map (LibreTranslate + gTTS use ISO codes)
# You can expand this map to whatever you need.
# -----------------
LANGS = {
    "English": "en",
    "Korean": "ko",
    "Vietnamese": "vi",
    "Burmese": "my",
    "Khmer": "km",
    "Chinese (Simplified)": "zh",
    "Japanese": "ja",
    "French": "fr",
    "Spanish": "es"
}

# helper to map for Google recognizer language tags
RECOGNIZER_LANG_TAG = {
    "ko": "ko-KR",
    "en": "en-US",
    "vi": "vi-VN",
    "my": "my-MM",   # might not be supported by recognizer
    "km": "km-KH",   # might not be supported
    "zh": "zh-CN",
    "ja": "ja-JP",
    "fr": "fr-FR",
    "es": "es-ES"
}

# LibreTranslate endpoint (public instance)
LIBRETRANSLATE_URL = "https://libretranslate.com/translate"

# -----------------
# Worker functions (run in background threads)
# -----------------
def do_record_and_transcribe(source_lang_code="ko"):
    """Record from microphone (single phrase) and transcribe using Google's web recognizer."""
    signals.set_status.emit("Listening...")
    r = sr.Recognizer()
    with sr.Microphone() as source:
        r.adjust_for_ambient_noise(source, duration=0.5)
        try:
            audio = r.listen(source, timeout=8, phrase_time_limit=12)
        except Exception as e:
            signals.set_status.emit(f"Recording error: {e}")
            signals.transcription_ready.emit("")
            return

    signals.set_status.emit("Transcribing...")
    # recognizer language tag (e.g., ko-KR)
    lang_tag = RECOGNIZER_LANG_TAG.get(source_lang_code, "ko-KR")
    try:
        text = r.recognize_google(audio, language=lang_tag)
        signals.transcription_ready.emit(text)
        signals.set_status.emit("Transcription done.")
    except sr.UnknownValueError:
        signals.set_status.emit("Could not understand audio.")
        signals.transcription_ready.emit("")
    except sr.RequestError as e:
        signals.set_status.emit(f"STT request error: {e}")
        signals.transcription_ready.emit("")

def do_translate(text, source_code, target_code):
    signals.set_status.emit("Translating...")
    if not text:
        signals.set_status.emit("Nothing to translate.")
        signals.translation_ready.emit("")
        return
    try:
        payload = {
            "q": text,
            "source": source_code,
            "target": target_code,
            "format": "text"
        }
        resp = requests.post(LIBRETRANSLATE_URL, json=payload, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        translated = data.get("translatedText", "")
        signals.translation_ready.emit(translated)
        signals.set_status.emit("Translation done.")
    except Exception as e:
        signals.set_status.emit(f"Translation error: {e}")
        signals.translation_ready.emit("")

def do_tts_play(text, target_code):
    if not text:
        signals.set_status.emit("Nothing to play.")
        return
    signals.set_status.emit("Generating speech...")
    try:
        # gTTS language uses short ISO codes (en, ko, vi, ...)
        tts = gTTS(text=text, lang=target_code)
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tmp.close()
        tts.save(tmp.name)
        signals.set_status.emit("Playing audio...")
        playsound(tmp.name)
        try:
            os.remove(tmp.name)
        except:
            pass
        signals.set_status.emit("Done.")
    except Exception as e:
        signals.set_status.emit(f"TTS error: {e}")

# -----------------
# Main Window
# -----------------
class MainWindow(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Korean → Multilang Speech Translator (MCP)")
        self.setMinimumSize(600, 480)
        layout = QVBoxLayout()

        # status
        self.status_label = QLabel("Ready")
        layout.addWidget(self.status_label)

        # controls: target language selector + buttons
        controls = QHBoxLayout()
        self.lang_box = QComboBox()
        for name in LANGS.keys():
            if name == "Korean":
                continue  # target shouldn't be Korean by default
            self.lang_box.addItem(name)
        controls.addWidget(QLabel("Target:"))
        controls.addWidget(self.lang_box)

        self.record_button = QPushButton("Record (Korean)")
        self.record_button.clicked.connect(self.on_record_clicked)
        controls.addWidget(self.record_button)

        self.play_button = QPushButton("Play Translation")
        self.play_button.clicked.connect(self.on_play_clicked)
        controls.addWidget(self.play_button)

        layout.addLayout(controls)

        # text areas
        layout.addWidget(QLabel("Recognized (Korean):"))
        self.orig_text = QTextEdit()
        self.orig_text.setReadOnly(False)
        layout.addWidget(self.orig_text)

        layout.addWidget(QLabel("Translated:"))
        self.trans_text = QTextEdit()
        self.trans_text.setReadOnly(True)
        layout.addWidget(self.trans_text)

        self.setLayout(layout)

        # wire signals
        signals.transcription_ready.connect(self.on_transcription_ready)
        signals.translation_ready.connect(self.on_translation_ready)
        signals.set_status.connect(self.on_status)

    def on_status(self, s):
        self.status_label.setText(s)

    def on_record_clicked(self):
        # disable button while recording
        self.record_button.setEnabled(False)
        self.status_label.setText("Starting...")
        # Record & transcribe in background
        t = threading.Thread(target=self.background_record_and_translate, daemon=True)
        t.start()

    def background_record_and_translate(self):
        # 1) STT (assume source = Korean)
        do_record_and_transcribe(source_lang_code="ko")
        # transcription_ready signal will trigger translation below

    def on_transcription_ready(self, text):
        self.orig_text.setPlainText(text)
        # after transcription, call translation
        target_name = self.lang_box.currentText()
        target_code = LANGS.get(target_name, "en")
        # run translation in background thread
        t = threading.Thread(target=do_translate, args=(text, "ko", target_code), daemon=True)
        t.start()
        # re-enable record button
        self.record_button.setEnabled(True)

    def on_translation_ready(self, translated):
        self.trans_text.setPlainText(translated)

    def on_play_clicked(self):
        translated = self.trans_text.toPlainText().strip()
        if not translated:
            self.status_label.setText("No translated text to play.")
            return
        target_name = self.lang_box.currentText()
        target_code = LANGS.get(target_name, "en")
        t = threading.Thread(target=do_tts_play, args=(translated, target_code), daemon=True)
        t.start()

# -----------------
# Run
# -----------------
def main():
    app = QApplication(sys.argv)
    w = MainWindow()
    w.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main()
```

---

# E. How to test (quick)

1. Start the script: `python stt_translate_pyqt.py`
2. Select a target language (e.g., English).
3. Click **Record** and speak in Korean (short phrase). The UI will show recognized Korean text and the translated text, and you can press **Play** to hear the translated speech.

---

# F. Next improvements & production notes

* Replace Google quick recognizer with **Vosk** (offline) or **whisper/whisper.cpp** for better multilingual STT and privacy. Vosk has Korean models (e.g., `vosk-model-small-ko-0.22`) you can download and use locally. ([Alpha Cephei][3])
* LibreTranslate public instance is great for prototyping — self-host a LibreTranslate server for production or use commercial APIs for better quality/coverage. ([libretranslate.com][1])
* For better TTS voices, consider Coqui TTS / Hugging Face models or commercial cloud TTS (higher-quality, more natural Korean voices). ([GitHub][6])
* Add language auto-detection, streaming partial transcripts, and non-blocking audio playback with a proper audio library (e.g., `pydub` + `simpleaudio`) for robustness.

---

# G. Security / legal / limits

* Public translation & TTS services have rate limits and privacy considerations — don't send sensitive audio/text to public endpoints in production. Consider self-hosting models if the data is private. ([portal.libretranslate.com][2])

---

If you want, I can **(pick one)** right now:

1. Convert the above prototype to use **Vosk** (offline STT) and give you exact steps to download the Korean model + updated code. ([Alpha Cephei][3])
2. Replace simple `playsound` TTS playback with a non-blocking audio player (pydub + simpleaudio) and cross-platform packaging instructions.
3. Design the backend API contract (endpoints + JSON schema) that your future backend should implement so the frontend can swap to it later (ideal for Step 3).

Pick one and I’ll deliver the full next step (I’ll assume you want option 1 if you don’t choose).

[1]: https://libretranslate.com/ "LibreTranslate - Free and Open Source Machine Translation API "
[2]: https://portal.libretranslate.com/?utm_source=chatgpt.com "LibreTranslate Portal - Get API Key"
[3]: https://alphacephei.com/vosk/?utm_source=chatgpt.com "VOSK Offline Speech Recognition API"
[4]: https://pypi.org/project/SpeechRecognition/?utm_source=chatgpt.com "SpeechRecognition"
[5]: https://pypi.org/project/gTTS/1.1.5/?utm_source=chatgpt.com "gTTS 1.1.5"
[6]: https://github.com/idiap/coqui-ai-TTS?utm_source=chatgpt.com "idiap/coqui-ai-TTS: 🐸💬 - a deep learning toolkit for Text-to- ..."
